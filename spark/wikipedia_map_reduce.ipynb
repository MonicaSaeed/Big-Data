{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PopularWikipediaPagesMapReduce\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    try:\n",
    "        fields = line.split()\n",
    "        project = fields[0]\n",
    "        title = fields[1]\n",
    "        hits = int(fields[2])\n",
    "        size = int(fields[3])\n",
    "        return (project, title, hits, size)\n",
    "    except Exception as e:\n",
    "        # print(\"Error parsing line:\", line)\n",
    "        # print(\"Exception:\", e)\n",
    "        return None\n",
    "\n",
    "# Read the data from the file using parseLine function\n",
    "lines = sc.textFile(\"pagecounts-20160101-000000_parsed.out\")\n",
    "rdd = lines.map(parseLine).filter(lambda x: x is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute min, max, and average page sizes\n",
    "minSize = rdd.map(lambda x: x[3]).min()\n",
    "maxSize = rdd.map(lambda x: x[3]).max()\n",
    "avgSize = rdd.map(lambda x: x[3]).mean()\n",
    "\n",
    "# print(\"Min page size: {}, Max page size: {}, Average page size: {}\".format(minSize, maxSize, avgSize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count page titles that start with \"The\"\n",
    "# the_titles_count = rdd.filter(lambda x: x[1].startswith(\"The\")).count()\n",
    "\n",
    "# Count page titles that start with \"The\" and are not part of the English project\n",
    "english_the_titles_count = rdd.filter(lambda x: x[0] != \"en\" and x[1].startswith(\"The\")).count()\n",
    "# print(\"The titles count: {}, English The titles count: {}\".format(the_titles_count, english_the_titles_count))\n",
    "\n",
    "# print(\"English The titles count: {}\".format(english_the_titles_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['271_ac', 'categoryuser_th', 'chiron_elias_krase', 'dassault_rafaele', 'edesv']\n",
      "['271', 'ac', 'categoryuser', 'th', 'chiron']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_title(title):\n",
    "    # Lowercase the title\n",
    "    title = title.lower()\n",
    "    # Remove non-alphanumeric characters an\n",
    "    title = re.sub(r'[^a-z0-9_]', '', title)\n",
    "    return title\n",
    "\n",
    "# Preprocess page titles\n",
    "preprocessed_titles = rdd.map(lambda x: preprocess_title(x[1]))\n",
    "print(preprocessed_titles.take(5))\n",
    "\n",
    "# Split titles into terms and flatten\n",
    "terms = preprocessed_titles.flatMap(lambda title: title.split(\"_\"))\n",
    "print(terms.take(5))\n",
    "\n",
    "# Count unique terms\n",
    "unique_terms_count = terms.distinct().count()\n",
    "\n",
    "# print(\"Number of unique terms appearing in the page titles:\", unique_terms_count)\n",
    "\n",
    "\n",
    "# re.sub(): This function is used for performing substitutions based on regular expressions. It takes three main arguments:\n",
    "# The first argument is the regular expression pattern to search for.\n",
    "# The second argument is the replacement string, which will replace the matched pattern.\n",
    "# The third argument is the string on which the operation is performed.\n",
    "# r'[^a-zA-Z0-9_]': This regular expression pattern matches any character that is not alphanumeric (a-z, A-Z, 0-9) or an underscore _.\n",
    "# '': This is the replacement string, which is an empty string. It means that any character that matches the pattern will be removed (replaced with nothing).\n",
    "# 'title: This is the string on which the substitution operation is performed. In this case, it's the page title.\n",
    "# So, the re.sub() function will remove all characters from the page title that are not alphanumeric or underscores.\n",
    "\n",
    "# [\"hello world\", \"foo bar\", \"baz\"]\n",
    "# map\n",
    "# [\"hello\", \"world\"]\n",
    "# [\"foo\", \"bar\"]\n",
    "# [\"baz\"]\n",
    "# flatMap\n",
    "# [\"hello\", \"world\", \"foo\", \"bar\", \"baz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract each title and the number of times it was repeated\n",
    "title_counts = rdd.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# for title, count in title_counts.take(10):\n",
    "#     print(\"Title: {}, Count: {}\".format(title, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine between data of pages with the same title and save each pair of pages data in order to display them\n",
    "# combined_titles = rdd.map(lambda x: (x[1], (x[0], x[2], x[3]))).groupByKey().mapValues(list)\n",
    "combined_titles = rdd.map(lambda x: (x[1], (x[0], x[2], x[3]))).groupByKey().filter(lambda x: len(x[1]) > 1).mapValues(list)\n",
    "\n",
    "# print(\"Combined titles:\")\n",
    "# for title, data in combined_titles.take(10):\n",
    "#     print(\"Title: {}, Data: {}\".format(title, data))\n",
    "\n",
    "# mapValues(list) to convert the iterator of values into a list. This will give you each title paired with a list of its corresponding data tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document includes all the results of each query\n",
    "with open(\"map_reduce_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Min page size: {}\\n\".format(minSize))\n",
    "    f.write(\"Max page size: {}\\n\".format(maxSize))\n",
    "    f.write(\"Average page size: {}\\n\".format(avgSize))\n",
    "    f.write(\"English The titles count: {}\\n\".format(english_the_titles_count))\n",
    "    f.write(\"Number of unique terms appearing in the page titles: {}\\n\".format(unique_terms_count))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Title Counts:\\n\")\n",
    "    for title, count in title_counts.collect():\n",
    "        f.write(\"{}: {}\\n\".format(title, count))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Combined Titles:\\n\")\n",
    "    for title, data_list in combined_titles.collect():\n",
    "        f.write(\"{}:\\n\".format(title))\n",
    "        for data in data_list:\n",
    "            f.write(\"{}\\n\".format(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9b07baca1d7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Stop the existing SparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Stop the existing SparkContext\n",
    "sc.stop()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:  232.958838224411\n",
      "Total time in minutes:  3.8826473037401836\n"
     ]
    }
   ],
   "source": [
    "total_time = end_time - Start_time\n",
    "print(\"Total time: \", total_time)\n",
    "total_time_minutes = total_time / 60\n",
    "print(\"Total time in minutes: \", total_time_minutes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
